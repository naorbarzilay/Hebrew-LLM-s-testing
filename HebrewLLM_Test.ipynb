{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "P0v94Yqk6y86",
        "n4AUk0eHEqC_",
        "fb1kOZdJYXFQ",
        "E_4pkSMMehxB",
        "nk_3Ihzwl13N"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hebrew LLM Test"
      ],
      "metadata": {
        "id": "BJT_4d7z6t3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests google-generativeai"
      ],
      "metadata": {
        "id": "6FGJlBUcGKbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Winograd"
      ],
      "metadata": {
        "id": "P0v94Yqk6y86"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2tAdR076qNO",
        "outputId": "661bb612-b0fb-40b5-fb3a-c00517363fa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Winograd evaluation on 5 samples\n",
            "\n",
            "Sample 1\n",
            "Context: חברי המועצה לא נתנו אישור למפגינים כי הם צפו אלימות.\n",
            "Question: מי צפו אלימות?\n",
            "Model answer: חברי המועצה\n",
            "Gold answer:  חברי המועצה\n",
            "Correct: True\n",
            "------------------------------------------------------------\n",
            "Sample 2\n",
            "Context: חברי המועצה לא נתנו אישור למפגינים כי הם עודדו אלימות.\n",
            "Question: מי עודדו אלימות?\n",
            "Model answer: המפגינים\n",
            "Gold answer:  המפגינים\n",
            "Correct: True\n",
            "------------------------------------------------------------\n",
            "Sample 3\n",
            "Context: הגביע לא נכנס לתיק החום כי הוא קטן מדי.\n",
            "Question: מה קטן מדי?\n",
            "Model answer: **התיק**\n",
            "Gold answer:  התיק\n",
            "Correct: False\n",
            "------------------------------------------------------------\n",
            "Sample 4\n",
            "Context: הגביע לא נכנס לתיק החום כי הוא גדול מדי.\n",
            "Question: מה גדול מדי?\n",
            "Model answer: הגביע\n",
            "Gold answer:  הגביע\n",
            "Correct: True\n",
            "------------------------------------------------------------\n",
            "Sample 5\n",
            "Context: יעל הודתה למיכל על כל הסיוע שהיא העניקה.\n",
            "Question: מי הודתה למי על כל הסיוע שהיא העניקה?\n",
            "Model answer: יעל\n",
            "Gold answer:  מיכל\n",
            "Correct: False\n",
            "------------------------------------------------------------\n",
            "\n",
            "FINAL ACCURACY: 0.600 (3/5)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "import requests\n",
        "from typing import List, Dict\n",
        "\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "API_KEY = \"YOUR API KEY\"\n",
        "DATASET_PATH = \"winograd_he.jsonl\"\n",
        "\n",
        "API_URL = \"https://api.perplexity.ai/chat/completions\"\n",
        "MODEL_NAME = \"sonar-pro\"   # אפשר גם sonar-small / sonar-medium\n",
        "TEMPERATURE = 0.0\n",
        "SLEEP_BETWEEN_CALLS = 1.0  # הגנה מ-rate limit\n",
        "\n",
        "\n",
        "# =========================\n",
        "# FEW-SHOT PROMPT\n",
        "# =========================\n",
        "FEW_SHOT_PROMPT = \"\"\"אתה מבצע מבחן הבנת שפה מסוג Winograd בעברית.\n",
        "\n",
        "בכל שאלה:\n",
        "- יופיע משפט\n",
        "- תישאל שאלה\n",
        "- יוצגו שתי אפשרויות תשובה\n",
        "\n",
        "עליך לבחור איזו אפשרות היא התשובה הנכונה.\n",
        "ענה רק בטקסט של אחת האפשרויות, ללא הסבר.\n",
        "\n",
        "דוגמה 1:\n",
        "משפט: הילד לא הצליח להרים את התרמיל כי הוא היה כבד מדי.\n",
        "שאלה: מה היה כבד מדי?\n",
        "אפשרות 1: הילד\n",
        "אפשרות 2: התרמיל\n",
        "תשובה: התרמיל\n",
        "\n",
        "דוגמה 2:\n",
        "משפט: המורה גער בתלמיד כי הוא הפריע לשיעור.\n",
        "שאלה: מי הפריע לשיעור?\n",
        "אפשרות 1: המורה\n",
        "אפשרות 2: התלמיד\n",
        "תשובה: התלמיד\n",
        "\n",
        "דוגמה 3:\n",
        "משפט: הגביע לא נכנס לתיק כי הוא קטן מדי.\n",
        "שאלה: מה קטן מדי?\n",
        "אפשרות 1: הגביע\n",
        "אפשרות 2: התיק\n",
        "תשובה: התיק\n",
        "\n",
        "דוגמה 4:\n",
        "משפט: השוטר עצר את הנהג כי הוא עבר באור אדום.\n",
        "שאלה: מי עבר באור אדום?\n",
        "אפשרות 1: השוטר\n",
        "אפשרות 2: הנהג\n",
        "תשובה: הנהג\n",
        "\n",
        "דוגמה 5:\n",
        "משפט: חברי הוועדה לא אישרו את הבקשה כי הם חששו מתוצאותיה.\n",
        "שאלה: מי חששו מהתוצאות?\n",
        "אפשרות 1: חברי הוועדה\n",
        "אפשרות 2: הבקשה\n",
        "תשובה: חברי הוועדה\n",
        "\n",
        "כעת ענה על השאלה הבאה:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# =========================\n",
        "# UTILS\n",
        "# =========================\n",
        "def load_dataset(path: str, limit: int = 5) -> List[Dict]:\n",
        "    samples = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= limit:\n",
        "                break\n",
        "            samples.append(json.loads(line))\n",
        "    return samples\n",
        "\n",
        "\n",
        "def build_prompt(sample: Dict) -> str:\n",
        "    return (\n",
        "        FEW_SHOT_PROMPT\n",
        "        + f\"\\nמשפט: {sample['context']}\\n\"\n",
        "        + f\"שאלה: {sample['question']}\\n\"\n",
        "        + f\"אפשרות 1: {sample['option1']}\\n\"\n",
        "        + f\"אפשרות 2: {sample['option2']}\\n\"\n",
        "        + \"תשובה:\"\n",
        "    )\n",
        "\n",
        "\n",
        "def call_perplexity(prompt: str) -> str:\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a precise Hebrew language understanding model.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"temperature\": TEMPERATURE\n",
        "    }\n",
        "\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    data = response.json()\n",
        "    return data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "\n",
        "def normalize_answer(text: str) -> str:\n",
        "    return text.replace(\"\\n\", \"\").strip()\n",
        "\n",
        "\n",
        "# =========================\n",
        "# MAIN EVALUATION\n",
        "# =========================\n",
        "def run_evaluation():\n",
        "    dataset = load_dataset(DATASET_PATH)\n",
        "\n",
        "    correct = 0\n",
        "    total = len(dataset)\n",
        "\n",
        "    print(f\"Running Winograd evaluation on {total} samples\\n\")\n",
        "\n",
        "    for idx, sample in enumerate(dataset, start=1):\n",
        "        prompt = build_prompt(sample)\n",
        "\n",
        "        model_answer = normalize_answer(call_perplexity(prompt))\n",
        "\n",
        "        gold_answer = (\n",
        "            sample[\"option1\"]\n",
        "            if sample[\"label\"] == 1\n",
        "            else sample[\"option2\"]\n",
        "        )\n",
        "\n",
        "        is_correct = model_answer == gold_answer\n",
        "        correct += int(is_correct)\n",
        "\n",
        "        print(f\"Sample {idx}\")\n",
        "        print(f\"Context: {sample['context']}\")\n",
        "        print(f\"Question: {sample['question']}\")\n",
        "        print(f\"Model answer: {model_answer}\")\n",
        "        print(f\"Gold answer:  {gold_answer}\")\n",
        "        print(f\"Correct: {is_correct}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        time.sleep(SLEEP_BETWEEN_CALLS)\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0.0\n",
        "    print(f\"\\nFINAL ACCURACY: {accuracy:.3f} ({correct}/{total})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_evaluation()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarization Task"
      ],
      "metadata": {
        "id": "n4AUk0eHEqC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" style=\"direction: rtl; text-align: right;\">\n",
        "  בחלק הזה משווים בין תשובת המודל הנבחן לבין התשובה הנכונה (gold),\n",
        "  בעזרת מודל שיפוט (LLM as a Judge).  \n",
        "  לא מצאתי את המסד נתונים שיעקב ציין אז נצטרך להוסיף אותו או לא להשתמש במסד הנתונים ופשוט לבחון עם Zero Shot\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "Hs_DOfG0EspX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PERPLEXITY_API_KEY = \"YOUR API KEY\"\n",
        "GEMINI_API_KEY = \"YOUR API KEY\"\n"
      ],
      "metadata": {
        "id": "pz_coYY5BMm-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import google.generativeai as genai\n",
        "from typing import Dict, List\n",
        "\n",
        "\n",
        "DATASET_PATH = \"hebrew_summaries_small.jsonl\"\n",
        "\n",
        "PERPLEXITY_MODEL = \"sonar-pro\"\n",
        "PERPLEXITY_URL = \"https://api.perplexity.ai/chat/completions\"\n",
        "\n",
        "GEMINI_MODEL = \"models/gemini-2.5-flash\"\n",
        "\n",
        "SLEEP = 1.0\n",
        "\n",
        "\n",
        "# =========================\n",
        "# GEMINI INIT (JUDGE)\n",
        "# =========================\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "judge_model = genai.GenerativeModel(GEMINI_MODEL)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# PROMPTS\n",
        "# =========================\n",
        "SUMMARIZATION_PROMPT = \"\"\"סכם את המסמך הבא בעברית.\n",
        "\n",
        "הנחיות:\n",
        "- שמור על עיקרי הדברים בלבד\n",
        "- אל תוסיף מידע שלא מופיע במסמך\n",
        "- כתוב סיכום קוהרנטי, ברור וענייני\n",
        "\n",
        "מסמך:\n",
        "{document}\n",
        "\n",
        "סיכום:\n",
        "\"\"\"\n",
        "\n",
        "# קריטריונים וצעדים (מבוסס על G-Eval)\n",
        "EVALUATION_CRITERIA = {\n",
        "    \"Relevance\": (\n",
        "        \"\"\"Relevance (1-10) – עד כמה הסיכום מכסה את עיקרי המסמך.\n",
        "סיכומים שמכילים מידע מיותר או חזרות צריכים להיקנס.\"\"\",\n",
        "        \"\"\"1. קרא את המסמך ואת הסיכום בקפידה.\n",
        "2. זהה את הנקודות המרכזיות במסמך.\n",
        "3. בדוק עד כמה הסיכום כולל את כל הנקודות החשובות ומצמצם מידע מיותר.\n",
        "4. הענק ציון בין 1 ל-10.\"\"\"\n",
        "    ),\n",
        "    \"Coherence\": (\n",
        "        \"\"\"Coherence (1-10) – איכות מבנה הסיכום והקוהרנטיות שלו.\n",
        "סיכום קוהרנטי מציג את המידע בצורה ברורה והגיונית.\"\"\",\n",
        "        \"\"\"1. קרא את הסיכום והשווה למסמך.\n",
        "2. בדוק אם הסיכום מציג את המידע בסדר לוגי וברור.\n",
        "3. הענק ציון בין 1 ל-10.\"\"\"\n",
        "    ),\n",
        "    \"Faithfulness\": (\n",
        "        \"\"\"Faithfulness (1-10) – עד כמה הסיכום נאמן למקור, ללא המצאות או טעויות עובדתיות.\"\"\",\n",
        "        \"\"\"1. קרא את המסמך ואת הסיכום.\n",
        "2. בדוק אם כל העובדות בסיכום מופיעות במקור.\n",
        "3. הענק ציון בין 1 ל-10.\"\"\"\n",
        "    ),\n",
        "    \"Fluency\": (\n",
        "        \"\"\"Fluency (1-10) – שטף, איות, דקדוק ואיכות השפה בעברית.\"\"\",\n",
        "        \"\"\"1. קרא את הסיכום.\n",
        "2. בדוק את בהירות השפה, דקדוק, פיסוק ואיות.\n",
        "3. הענק ציון בין 1 ל-10.\"\"\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
        "אתה מודל שיפוט אובייקטיבי להערכת סיכומים בעברית.\n",
        "\n",
        "קריטריוני הערכה:\n",
        "\n",
        "{criteria}\n",
        "\n",
        "צעדים להערכה:\n",
        "\n",
        "{steps}\n",
        "\n",
        "מסמך מקור:\n",
        "{document}\n",
        "\n",
        "סיכום מודל:\n",
        "{summary}\n",
        "\n",
        "החזר תוצאה במספרים בלבד, בפורמט JSON:\n",
        "{{\n",
        "  \"score\": מספר בין 1 ל-10\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# =========================\n",
        "# UTILS\n",
        "# =========================\n",
        "def load_dataset(path: str, limit: int = None) -> List[Dict]:\n",
        "    samples = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f, start=1):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                samples.append(json.loads(line))\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"⚠️ שגיאה בשורה {i}: {e}\")\n",
        "                print(\"השורה הבעייתית:\", line)\n",
        "                continue\n",
        "            if limit and len(samples) >= limit:\n",
        "                break\n",
        "    return samples\n",
        "\n",
        "def call_perplexity(prompt: str) -> str:\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {PERPLEXITY_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": PERPLEXITY_MODEL,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a precise Hebrew summarization model.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"temperature\": 0.0\n",
        "    }\n",
        "    r = requests.post(PERPLEXITY_URL, headers=headers, json=payload)\n",
        "    r.raise_for_status()\n",
        "    return r.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "def extract_json(text: str) -> dict:\n",
        "    \"\"\"\n",
        "    מחלץ JSON מתוך טקסט. מחזיר None אם לא נמצא.\n",
        "    \"\"\"\n",
        "    match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group())\n",
        "        except json.JSONDecodeError:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def call_gemini_judge(criteria: str, steps: str, document: str, summary: str) -> Dict:\n",
        "    prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
        "        criteria=criteria,\n",
        "        steps=steps,\n",
        "        document=document,\n",
        "        summary=summary\n",
        "    )\n",
        "    response = judge_model.generate_content(prompt)\n",
        "    score_dict = extract_json(response.text)\n",
        "    print(score_dict)\n",
        "    if score_dict is None:\n",
        "        print(\"⚠️ שגיאת JSON מהשופט, מחזיר None\")\n",
        "    return score_dict\n",
        "\n",
        "\n",
        "# =========================\n",
        "# MAIN EVAL\n",
        "# =========================\n",
        "def run_evaluation():\n",
        "    dataset = load_dataset(DATASET_PATH)\n",
        "    print(f\"Running evaluation on {len(dataset)} samples\\n\")\n",
        "    all_scores = []\n",
        "\n",
        "    for idx, sample in enumerate(dataset, 1):\n",
        "        print(f\"\\n===== Sample {idx} =====\")\n",
        "\n",
        "        # 1. Generate summary (Zero-shot)\n",
        "        summary_prompt = SUMMARIZATION_PROMPT.format(\n",
        "            document=sample[\"document\"]\n",
        "        )\n",
        "        model_summary = call_perplexity(summary_prompt)\n",
        "        print(model_summary)\n",
        "        # 2. Judge for each metric\n",
        "        sample_scores = {}\n",
        "        for metric, (criteria, steps) in EVALUATION_CRITERIA.items():\n",
        "            score_dict = call_gemini_judge(criteria, steps, sample[\"document\"], model_summary)\n",
        "            sample_scores[metric] = score_dict[\"score\"] if score_dict else None\n",
        "\n",
        "        print(\"here\")\n",
        "        # 3. Overall average\n",
        "        valid_scores = [v for v in sample_scores.values() if v is not None]\n",
        "        sample_scores[\"Overall\"] = round(sum(valid_scores)/len(valid_scores), 2) if valid_scores else None\n",
        "\n",
        "        all_scores.append(sample_scores)\n",
        "\n",
        "        print(\"Model summary:\\n\", model_summary)\n",
        "        print(\"Scores:\", sample_scores)\n",
        "\n",
        "        time.sleep(SLEEP)\n",
        "\n",
        "    # Aggregate averages across all samples\n",
        "    avg = {}\n",
        "    for metric in all_scores[0]:\n",
        "        vals = [s[metric] for s in all_scores if s[metric] is not None]\n",
        "        avg[metric] = round(sum(vals)/len(vals), 2) if vals else None\n",
        "\n",
        "    print(\"\\n===== FINAL AVERAGES =====\")\n",
        "    for k, v in avg.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "# =========================\n",
        "# RUN\n",
        "# =========================\n",
        "if __name__ == \"__main__\":\n",
        "    run_evaluation()\n"
      ],
      "metadata": {
        "id": "oqokAnaxYSGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long-Context NLI - Reasoning"
      ],
      "metadata": {
        "id": "fb1kOZdJYXFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from typing import List, Dict\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "DATASET_PATH = \"dikta_heb_test.json\"  # הנתונים שלך\n",
        "PERPLEXITY_MODEL = \"sonar-pro\"\n",
        "PERPLEXITY_URL = \"https://api.perplexity.ai/chat/completions\"\n",
        "SLEEP = 1.0  # זמן המתנה בין קריאות API\n",
        "\n",
        "# API Key\n",
        "PERPLEXITY_API_KEY  = \"YOUR API KEY\"\n",
        "\n",
        "\n",
        "# =========================\n",
        "# LOAD DATASET\n",
        "# =========================\n",
        "def load_dataset(path: str, limit: int = None) -> List[Dict]:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    if limit:\n",
        "        return data[:limit]\n",
        "    return data\n",
        "\n",
        "\n",
        "# =========================\n",
        "# CALL PERPLEXITY WITH REASONING\n",
        "# =========================\n",
        "\n",
        "def call_perplexity_nli(premise: str, hypothesis: str) -> str:\n",
        "    few_shot_example = \"\"\"\n",
        "דוגמה:\n",
        "\n",
        "  \"premise\": \"במהלך המאה ה-20 חלו שינויים משמעותיים בתנאי העבודה במדינות המערב. אף על פי שלא חסרו בעיות נלוות, ניתן לראות את השינויים הללו באופן כללי כחיוביים: עבודת ילדים כמעט פסקה, השכר עלה, מספר שעות העבודה בשבוע פחת, מדיניות הפנסיה הפכה לסטנדרטית, הטבות נלוות התרבו, ודאגה לבריאות ולבטיחות בעבודה הפכה למחייבת. איסוף נתונים על תנאי העבודה הפך למדע מדויק הרבה יותר. במיוחד חלו התפתחויות חשובות בשיטות איסוף הנתונים. בנוסף, חלה התרחבות משמעותית במאמץ איסוף הנתונים, יותר אנשים היו מעורבים בלמידה על מקום העבודה; ולראשונה, החלו להתפרסם תוצאות. כתוצאה מכך, בסוף המאה, לא רק שרוב העובדים היו במצב טוב יותר מקודמיהם בתחילת המאה ה-20, אלא הם גם היו בעמדה להבין כיצד ומדוע זה היה המקרה. על ידי ניתוח קפדני של הנתונים הסטטיסטיים שהיו זמינים, שינויים ספציפיים במקום העבודה לא פחות מאשר בנוגע למושג מה העבודה צריכה לכלול הפכו ברורים. השינויים הבולטים ביותר בסביבת העבודה נגעו לגודל ולמבנה של כוח העבודה. בארצות הברית, למשל, גדל כוח העבודה מ-24 מיליון (כולל עובדים מגיל עשר ומעלה) ל-139 מיליון (מגיל 16 ומעלה), כמעט פי שישה, בהתאם לגידול באוכלוסייה הכללית. באותה עת, הרכב כוח העבודה השתנה מתעשיות שעיקרן ייצור חקלאי, כמו חקלאים ויערנים, לתעשיות שעיקרן מקצועות חופשיים, טכניים ובמיוחד שירותים. בתחילת המאה ה־20, 38% מכלל העובדים האמריקנים הועסקו בחקלאות, בסופה של אותה מאה, שיעור זה צנח לפחות מ־3%. באירופה, תהליך דומה התרחש. בשנות ה־30 של המאה ה־20, בכל מדינה אירופית, פרט לבריטניה ובלגיה, יותר מ־20% מהאוכלוסייה עסקו בחקלאות. בשנות ה־80, לעומת זאת, אוכלוסיית החקלאים בכל המדינות המפותחות, למעט מזרח אירופה, צנחה ל־10% ולעתים אף פחות מכך. באותה עת, החקלאות האינטנסיבית, שהשתמשה בטכניקות ממוכנות, צמצמה באופן דרמטי את מספר העובדים הנדרשים לעבודה. וכאן טמונה הבעיה. בעוד מקום העבודה הפך לסביבה בטוחה ופרודוקטיבית יותר, הרחק מתנאי העבודה הקשים של אבותינו, המעבר מחקלאות לעבודה מודרנית יצר גם אבטלה המונית במדינות רבות. בלב הבעיה עמד המעבר מהכפר לעיר. לאחר שאיבדו את פרנסתם, התקבצו אוכלוסיות האיכרים במספרים הולכים וגדלים בקהילות צפופות, שבהן שיעורי התעסוקה לא הצליחו להדביק את קצב ההגירה הפנימית. כתוצאה מכך, אלפים נותרו יושבים במעברות בפאתי הערים, ממתינים למשרות שאולי לעולם לא יגיעו. בעוד שתופעה זו (ועדיין) אופיינית למדינות העולם השלישי, ניתן היה להבחין בה גם בכמה ערים אמריקאיות, צרפתיות, אנגליות וגרמניות בסוף המאה ה־20. מנקודת מבט שונה וחיובית, במאה ה-20 נשים הפכו לחברות פעילות ונראות בכל תחומי שוק העבודה המערבי. בשנת 1900, רק 19% מהנשים האירופאיות בגיל העבודה השתתפו בכוח העבודה; בשנת 1999, נתון זה עלה ל-60%. בשנת 1900, רק 1% מעורכי הדין במדינה ו-6% מהרופאים היו נשים; לעומת זאת, הנתונים היו 29% ו-24% בשנת 1999. סקר שנערך לאחרונה בקרב בני נוער צרפתים, גברים ונשים כאחד, העלה כי למעלה מ-50% מהנשאלים סבורים כי בכל עבודה (למעט זו הכרוכה בשירות צבאי) נשים הן עובדות טובות יותר, משום שהן נוטות פחות להתרגז תחת לחץ, ופחות תחרותיות מגברים. השינוי האחרון והאולי משמעותי ביותר במקומות העבודה של המאה ה-20 היה הכנסת הטכנולוגיה. רשימת השיפורים הטכנולוגיים במקומות העבודה היא אינסופית: מכשירי תקשורת ומדידה, מחשבים בכל הצורות והגדלים, רנטגן, לייזרים, אורות ניאון, פלדת אל-חלד וכן הלאה וכן הלאה. שיפורים אלה הובילו לסביבת עבודה יצרנית ובטוחה יותר. יתרה מכך, העובדה שהרפואה השתפרה באופן דרמטי כל כך הובילה לעלייה בתוחלת החיים בקרב אוכלוסיות מערביות. בתורן, עובדים בגילאים שונים מאוד יכלו לעבוד כתף אל כתף, ולהמשיך בעבודתם במשך שנים רבות יותר. בסוף המאה ה-20, סביבת העבודה המערבית עברה שינויים ניכרים. באופן כללי, הן גברים והן נשים עבדו פחות שעות ביום במשך שנים רבות יותר בתנאים טובים יותר. עם זאת, כוחה של החקלאות נחלש כאשר חקלאים ויערנים עברו לערים כדי להרוויח משכורות גבוהות יותר כסטטיסטיקאים ורואי חשבון. עבור אלה שלא יכלו לעשות את המעבר הזה, החיים עם שחר המאה החדשה נראו פחות מושכים.\",\n",
        "  \"hypothesis\": \"שיפורים ברפואה הובילו לכך שעובדים מרוויחים יותר לאורך זמן.\",\n",
        "  \"label\": \"n\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "{few_shot_example}\n",
        "\n",
        "כעת עבור הטקסט הבא:\n",
        "\n",
        "Premise: {premise}\n",
        "Hypothesis: {hypothesis}\n",
        "\n",
        "בחר אחת מהתוויות בלבד:\n",
        "- E (Entailment) – נובע מהטקסט\n",
        "- C (Contradiction) – סותר את הטקסט\n",
        "- N (Neutral) – לא ניתן לקבוע מהטקסט\n",
        "\n",
        "⚠️ חובה: כתוב רק את האות הסופית בלבד (E, C או N), שום הסבר או טקסט נוסף.\n",
        "\"\"\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {PERPLEXITY_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": PERPLEXITY_MODEL,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a precise Hebrew NLI model with reasoning.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"temperature\": 0.0,\n",
        "        \"reasoning\": True\n",
        "    }\n",
        "\n",
        "    r = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=300)\n",
        "    r.raise_for_status()\n",
        "    text = r.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "    # מחלץ רק את האות האחרונה באנגלית (E, C, N)\n",
        "    label = None\n",
        "    for line in reversed(text.splitlines()):\n",
        "        line = line.strip().upper()\n",
        "        if line in [\"E\", \"C\", \"N\"]:\n",
        "            label = line\n",
        "            break\n",
        "    return label\n",
        "\n",
        "# =========================\n",
        "# RUN EVALUATION\n",
        "# =========================\n",
        "def run_nli_evaluation():\n",
        "    dataset = load_dataset(DATASET_PATH, limit=5)\n",
        "    correct = 0\n",
        "\n",
        "    for idx, sample in enumerate(dataset, 1):\n",
        "        print(f\"\\n===== Sample {idx} =====\")\n",
        "        premise = sample[\"premise\"]\n",
        "        hypothesis = sample[\"hypothesis\"]\n",
        "        true_label = sample[\"label\"]\n",
        "\n",
        "        predicted_label = call_perplexity_nli(premise, hypothesis)\n",
        "        is_correct = predicted_label.lower() == true_label.lower()\n",
        "        correct += is_correct\n",
        "\n",
        "        print(\"Premise:\", premise[:100], \"...\")\n",
        "        print(\"Hypothesis:\", hypothesis)\n",
        "        print(\"True Label:\", true_label)\n",
        "        print(\"Predicted Label:\", predicted_label)\n",
        "        print(\"Correct:\", is_correct)\n",
        "\n",
        "    accuracy = correct / len(dataset)\n",
        "    print(\"\\n===== FINAL ACCURACY =====\")\n",
        "    print(f\"Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# RUN\n",
        "# =========================\n",
        "if __name__ == \"__main__\":\n",
        "    run_nli_evaluation()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_tLeqk2Y9hM",
        "outputId": "e07ae2b1-d441-48fa-da08-6290f908bcd5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Sample 1 =====\n",
            "Premise: בלילה שבין הראשון לאוקטובר לבין השני, נשרף עד היסוד מועדון הקופקבנה. המשטרה מתייחסת לשריפה בחשדנות.  ...\n",
            "Hypothesis: אם חברת הביטוח תשלם את מלוא הסכום, ג'ון הודג'ס ירוויח מהשריפה.\n",
            "True Label: n\n",
            "Predicted Label: E\n",
            "Correct: False\n",
            "\n",
            "===== Sample 2 =====\n",
            "Premise: קטע זה מספק מידע על סבסוד אנרגיה מתחדשת והשפעתו על השימוש בדלקים מאובנים. סוגיית סבסוד מקורות אנרגיה ...\n",
            "Hypothesis: סובסידיה ממשלתית עשויה להפחית את עלות האנרגיה המתחדשת\n",
            "True Label: e\n",
            "Predicted Label: E\n",
            "Correct: True\n",
            "\n",
            "===== Sample 3 =====\n",
            "Premise: קטע זה מספק מידע על סבסוד אנרגיה מתחדשת והשפעתו על השימוש בדלקים מאובנים. סוגיית סבסוד מקורות אנרגיה ...\n",
            "Hypothesis: דלקים מאובנים זולים יותר כיום מצורות של אנרגיה מתחדשת.\n",
            "True Label: e\n",
            "Predicted Label: E\n",
            "Correct: True\n",
            "\n",
            "===== Sample 4 =====\n",
            "Premise: קטע זה מספק מידע על סבסוד אנרגיה מתחדשת והשפעתו על השימוש בדלקים מאובנים. סוגיית סבסוד מקורות אנרגיה ...\n",
            "Hypothesis: הטמפרטורה הממוצעת בבריטניה צפויה לעלות ב־4% עד 2050.\n",
            "True Label: c\n",
            "Predicted Label: N\n",
            "Correct: False\n",
            "\n",
            "===== Sample 5 =====\n",
            "Premise: השנה רוב החנויות והרשתות מציעות פרסים והנחות על קניות כדי למשוך לקוחות. ...\n",
            "Hypothesis: שפע של סחורות יש, אבל המכירה אינה מתרוממת. אין שמחה ללקוחות.\n",
            "True Label: e\n",
            "Predicted Label: N\n",
            "Correct: False\n",
            "\n",
            "===== FINAL ACCURACY =====\n",
            "Accuracy: 40.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SNLI Accuracy"
      ],
      "metadata": {
        "id": "5HFPcQoSdvSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" style=\"direction: rtl; text-align: right;\">\n",
        "עובד בצורה זהה כמו ה- Long-Context NLI - Reasoning אך הפעם לא נכניס למודל את ה- reasoning = True כפרמטר.\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "v5j13XOjd_n2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic Analysis"
      ],
      "metadata": {
        "id": "E_4pkSMMehxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "from typing import List, Dict\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "DATASET_PATH = \"data.jsonl\"  # מסד הנתונים שלך\n",
        "PERPLEXITY_MODEL = \"sonar-pro\"\n",
        "PERPLEXITY_URL = \"https://api.perplexity.ai/chat/completions\"\n",
        "SLEEP = 1.0  # זמן המתנה בין קריאות API\n",
        "\n",
        "PERPLEXITY_API_KEY = \"YOUR API KEY\"\n",
        "\n",
        "# =========================\n",
        "# LOAD DATASET\n",
        "# =========================\n",
        "def load_dataset(path: str, limit: int = None) -> List[Dict]:\n",
        "    samples = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f, start=1):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                samples.append(json.loads(line))\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"⚠️ שגיאה בשורה {i}: {e}\")\n",
        "                continue\n",
        "            if limit and len(samples) >= limit:\n",
        "                break\n",
        "    return samples\n",
        "\n",
        "# =========================\n",
        "# CALL PERPLEXITY\n",
        "# =========================\n",
        "def call_perplexity_semantic(text: str) -> str:\n",
        "    \"\"\"\n",
        "    שולח קריאה למודל Perplexity לסיווג סנטימנט עם דוגמאות few-shot\n",
        "    \"\"\"\n",
        "    few_shot_examples = \"\"\"\n",
        "דוגמאות:\n",
        "\"איזה יום נפלא! אני מרגיש מדהים.\" -> Positive\n",
        "\"השירות היה גרוע, אין לי מילים.\" -> Negative\n",
        "\"המסמך כולל את כל המידע הנדרש.\" -> Neutral\n",
        "\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "אתה מתבקש לסווג טקסט קצר בעברית לפי סנטימנט רגשי. קיימות שלוש מחלקות:\n",
        "חיובי – הטקסט מביע רגש חיובי, שמחה, שביעות רצון או תמיכה.\n",
        "שלילי – הטקסט מביע רגש שלילי, כעס, אכזבה או תלונה.\n",
        "ניטרלי – הטקסט אינו מביע רגש ברור או מראה מידע עובדתי בלבד.\n",
        "\n",
        "{few_shot_examples}\n",
        "\n",
        "טקסט חדש לסיווג:\n",
        "\"{text}\"\n",
        "\n",
        "החזר רק את התיוג הסופי: Positive, Negative או Neutral.\n",
        "\"\"\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {PERPLEXITY_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": PERPLEXITY_MODEL,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a precise Hebrew sentiment analysis model.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"temperature\": 0.0\n",
        "    }\n",
        "\n",
        "    r = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=300)\n",
        "    r.raise_for_status()\n",
        "    return r.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "# =========================\n",
        "# EXTRACT LABEL\n",
        "# =========================\n",
        "def extract_label(text: str) -> str:\n",
        "    \"\"\"\n",
        "    מחלץ תיוג חוקי מתוך פלט המודל\n",
        "    \"\"\"\n",
        "    text_lower = text.lower()\n",
        "    if \"positive\" in text_lower:\n",
        "        return \"Positive\"\n",
        "    if \"negative\" in text_lower:\n",
        "        return \"Negative\"\n",
        "    if \"neutral\" in text_lower:\n",
        "        return \"Neutral\"\n",
        "    return \"Unknown\"\n",
        "\n",
        "# =========================\n",
        "# RUN EVALUATION\n",
        "# =========================\n",
        "def run_semantic_evaluation(limit: int = 5):\n",
        "    dataset = load_dataset(DATASET_PATH, limit=limit)\n",
        "    correct = 0\n",
        "\n",
        "    for idx, sample in enumerate(dataset, 1):\n",
        "        text = sample[\"text\"]\n",
        "        true_label = sample[\"tag_ids\"]\n",
        "\n",
        "        predicted_raw = call_perplexity_semantic(text)\n",
        "        predicted_label = extract_label(predicted_raw)\n",
        "        is_correct = predicted_label.lower() == true_label.lower()\n",
        "        correct += is_correct\n",
        "\n",
        "        print(f\"\\n===== Sample {idx} =====\")\n",
        "        print(\"Text:\", text[:100], \"...\")\n",
        "        print(\"True Label:\", true_label)\n",
        "        print(\"Predicted Label:\", predicted_label)\n",
        "        print(\"Correct:\", is_correct)\n",
        "\n",
        "    accuracy = correct / len(dataset)\n",
        "    print(\"\\n===== FINAL ACCURACY =====\")\n",
        "    print(f\"Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "# =========================\n",
        "# RUN\n",
        "# =========================\n",
        "if __name__ == \"__main__\":\n",
        "    run_semantic_evaluation(limit=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN9hEPiKdyAi",
        "outputId": "1be78ecc-4d5d-443d-d865-8bae6fb76fca"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Sample 1 =====\n",
            "Text: זה יותר קל להחזיק ביד ...\n",
            "True Label: Positive\n",
            "Predicted Label: Neutral\n",
            "Correct? False\n",
            "\n",
            "===== Sample 2 =====\n",
            "Text: בעיית הכלבים זה דבר מחריד ...\n",
            "True Label: Negative\n",
            "Predicted Label: Negative\n",
            "Correct? True\n",
            "\n",
            "===== Sample 3 =====\n",
            "Text: חזק טוב מספק פוזה אנשים אוהבים ריח לא רע טוב לגוינטים ...\n",
            "True Label: Positive\n",
            "Predicted Label: Positive\n",
            "Correct? True\n",
            "\n",
            "===== Sample 4 =====\n",
            "Text:  מקווה שהסקר הזה יועיל למחקר שלכם ...\n",
            "True Label: Neutral\n",
            "Predicted Label: Neutral\n",
            "Correct? True\n",
            "\n",
            "===== Sample 5 =====\n",
            "Text: נעים נינוח דק ...\n",
            "True Label: Positive\n",
            "Predicted Label: Positive\n",
            "Correct? True\n",
            "\n",
            "===== FINAL ACCURACY =====\n",
            "Accuracy: 80.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the current state of knowledge"
      ],
      "metadata": {
        "id": "nk_3Ihzwl13N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\" style=\"direction: rtl; text-align: right;\">\n",
        "מבצע בדיקה של עדכניות ידע של המודל, לפי החודשים מינואר 2023 ועד היום, ושומר את החודש והשנה האחרונה שהמודל מעודכן בה. הקוד מתמקד במודל  עם תשובות \"כן/לא\", ומוודא את החודש האחרון אם המודל מתחיל לענות \"לא\":\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "rzbUtP-zl5k0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "PERPLEXITY_MODEL = \"sonar-pro\"\n",
        "PERPLEXITY_URL = \"https://api.perplexity.ai/chat/completions\"\n",
        "PERPLEXITY_API_KEY = \"YOUR API KEY\"  # הכנס כאן את המפתח שלך\n",
        "SLEEP = 1.0  # זמן המתנה בין קריאות API\n",
        "\n",
        "# =========================\n",
        "# HELPER FUNCTIONS\n",
        "# =========================\n",
        "def call_perplexity_yes_no(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    שולח קריאה למודל Perplexity ושומר תשובה \"כן\" או \"לא\".\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {PERPLEXITY_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": PERPLEXITY_MODEL,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a precise Hebrew model that answers only Yes or No questions.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"temperature\": 0.0,\n",
        "        \"reasoning\": True  # מאפשר reasoning\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=120)\n",
        "        r.raise_for_status()\n",
        "        text = r.json()[\"choices\"][0][\"message\"][\"content\"].strip().lower()\n",
        "        if \"כן\" in text:\n",
        "            return \"כן\"\n",
        "        elif \"לא\" in text:\n",
        "            return \"לא\"\n",
        "        else:\n",
        "            return \"לא ידוע\"\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ שגיאה בקריאה ל-Perplexity:\", e)\n",
        "        return \"לא ידוע\"\n",
        "\n",
        "# =========================\n",
        "# MAIN FUNCTION\n",
        "# =========================\n",
        "def find_latest_knowledge_date():\n",
        "    start_year = 2023\n",
        "    start_month = 1\n",
        "    today = datetime.today()\n",
        "    current_year = today.year\n",
        "    current_month = today.month\n",
        "\n",
        "    latest_known_year = None\n",
        "    latest_known_month = None\n",
        "\n",
        "    # עבור כל חודש מההתחלה עד היום\n",
        "    for year in range(start_year, current_year + 1):\n",
        "        first_month = start_month if year == start_year else 1\n",
        "        last_month = current_month if year == current_year else 12\n",
        "        for month in range(first_month, last_month + 1):\n",
        "            month_str = f\"{month:02d}/{year}\"\n",
        "            prompt = f\"האם אתה מכיר אירועים, חדשות או מידע שנוגע לחודש {month_str}? ענה רק 'כן' או 'לא'.\"\n",
        "            answer = call_perplexity_yes_no(prompt)\n",
        "            print(f\"{month_str} -> {answer}\")\n",
        "\n",
        "            if answer == \"כן\":\n",
        "                latest_known_year = year\n",
        "                latest_known_month = month\n",
        "            elif answer == \"לא\":\n",
        "                # בדיקה נוספת לוודא שאין עדכון מעבר\n",
        "                prompt_check = f\"אנא אמת: האם המידע האחרון שאתה מכיר הוא מחודש {month_str}? ענה רק 'כן' או 'לא'.\"\n",
        "                confirm = call_perplexity_yes_no(prompt_check)\n",
        "                print(f\"  Confirmation -> {confirm}\")\n",
        "                if confirm == \"לא\":\n",
        "                    # עצור כאן – המודל לא מעודכן מעבר לחודש הקודם\n",
        "                    print(\"\\n📌 המודל מעודכן עד:\")\n",
        "                    print(f\"  שנה: {latest_known_year}, חודש: {latest_known_month:02d}\")\n",
        "                    return latest_known_year, latest_known_month\n",
        "            time.sleep(SLEEP)\n",
        "\n",
        "    # אם עברנו על כל החודשים ועדיין המודל עונה \"כן\"\n",
        "    print(\"\\n📌 המודל מעודכן עד היום (או החודש האחרון בבדיקה):\")\n",
        "    print(f\"  שנה: {latest_known_year}, חודש: {latest_known_month:02d}\")\n",
        "    return latest_known_year, latest_known_month\n",
        "\n",
        "# =========================\n",
        "# RUN\n",
        "# =========================\n",
        "if __name__ == \"__main__\":\n",
        "    year, month = find_latest_knowledge_date()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXxbxTzMl2q4",
        "outputId": "3c14a53b-8886-436b-e074-fd6e63a9356b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01/2023 -> כן\n",
            "02/2023 -> כן\n",
            "03/2023 -> כן\n",
            "04/2023 -> כן\n",
            "05/2023 -> כן\n",
            "06/2023 -> כן\n",
            "07/2023 -> כן\n",
            "08/2023 -> כן\n",
            "09/2023 -> כן\n",
            "10/2023 -> כן\n",
            "11/2023 -> כן\n",
            "12/2023 -> כן\n",
            "01/2024 -> כן\n",
            "02/2024 -> כן\n",
            "03/2024 -> כן\n",
            "04/2024 -> כן\n",
            "05/2024 -> כן\n",
            "06/2024 -> כן\n",
            "07/2024 -> כן\n",
            "08/2024 -> כן\n",
            "09/2024 -> כן\n",
            "10/2024 -> כן\n",
            "11/2024 -> כן\n",
            "12/2024 -> כן\n",
            "01/2025 -> כן\n",
            "02/2025 -> כן\n",
            "03/2025 -> כן\n",
            "04/2025 -> כן\n",
            "05/2025 -> כן\n",
            "06/2025 -> כן\n",
            "07/2025 -> כן\n",
            "08/2025 -> כן\n",
            "09/2025 -> לא\n",
            "  Confirmation -> לא\n",
            "\n",
            "📌 המודל מעודכן עד:\n",
            "  שנה: 2025, חודש: 08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hebrew Question Answering (HeQ)"
      ],
      "metadata": {
        "id": "ZUimj2XwyDvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ייתכן שנצטרך לעשות פילטר לשאלות לפי הסוג שאנחנו רוצים כפי שמופיע פה:\n",
        "\n",
        "https://github.com/NNLP-IL/Hebrew-Question-Answering-Dataset"
      ],
      "metadata": {
        "id": "Pr0Ptw6T4Kw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### פונקציה למימוש TLNLS"
      ],
      "metadata": {
        "id": "b5ZyV2e77hN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "הפונקציה נותנת את אותו ניקוד כפי שמופיע במאמר\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "OuPk3am07mym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def levenshtein_distance(s: str, t: str) -> int:\n",
        "    \"\"\"Compute Levenshtein edit distance between two strings.\"\"\"\n",
        "    m, n = len(s), len(t)\n",
        "    # initialize DP matrix\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            cost = 0 if s[i - 1] == t[j - 1] else 1\n",
        "            dp[i][j] = min(\n",
        "                dp[i - 1][j] + 1,      # deletion\n",
        "                dp[i][j - 1] + 1,      # insertion\n",
        "                dp[i - 1][j - 1] + cost   # substitution\n",
        "            )\n",
        "    return dp[m][n]\n",
        "\n",
        "def normalized_lev_similarity(a: str, b: str) -> float:\n",
        "    \"\"\"\n",
        "    Normalized Levenshtein Similarity between two tokens:\n",
        "    ls(a, b) = 1 - lev(a, b) / max(len(a), len(b))\n",
        "    \"\"\"\n",
        "    if len(a) == 0 and len(b) == 0:\n",
        "        return 1.0\n",
        "    lev = levenshtein_distance(a, b)\n",
        "    return 1.0 - lev / max(len(a), len(b))\n",
        "\n",
        "def token_level_normalized_lev_similarity(pred: str, gold: str) -> float:\n",
        "    \"\"\"\n",
        "    TLNLS between two Hebrew strings.\n",
        "    Tokenize by whitespace, compute similarity token-wise.\n",
        "    \"\"\"\n",
        "    # Tokenize inputs\n",
        "    pred_tokens = pred.split()\n",
        "    gold_tokens = gold.split()\n",
        "\n",
        "    if len(pred_tokens) == 0 and len(gold_tokens) == 0:\n",
        "        return 1.0\n",
        "\n",
        "    # For each gold token find best matching pred token\n",
        "    total_similarity = 0.0\n",
        "    for g in gold_tokens:\n",
        "        best = 0.0\n",
        "        for p in pred_tokens:\n",
        "            score = normalized_lev_similarity(g, p)\n",
        "            if score > best:\n",
        "                best = score\n",
        "        total_similarity += best\n",
        "\n",
        "    # Divide by maximum number of tokens\n",
        "    norm_factor = max(len(gold_tokens), len(pred_tokens))\n",
        "    return total_similarity / norm_factor\n",
        "\n",
        "\n",
        "pred = \"בית\"\n",
        "gold = \"ביתינו\"\n",
        "\n",
        "score = token_level_normalized_lev_similarity(pred, gold)\n",
        "print(f\"TLNLS similarity: {score:.4f}\")\n",
        "\n",
        "\n",
        "pred = \"המוזיאון\"\n",
        "gold = \"מוזיאון\"\n",
        "\n",
        "score = token_level_normalized_lev_similarity(pred, gold)\n",
        "print(f\"TLNLS similarity: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UQJ9lM67lfz",
        "outputId": "7bcd0939-554b-4374-ed96-d29b0f59fe71"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TLNLS similarity: 0.5000\n",
            "TLNLS similarity: 0.8750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_shortest_span(predicted_answer: str, true_answers: list) -> str:\n",
        "    \"\"\"\n",
        "    מנסה לחלץ מהתשובה של המודל את ה-span הקצר ביותר\n",
        "    שתואם לאחת התשובות האמיתיות.\n",
        "    אם לא מצליח, מחזיר את predicted_answer אחרי ניקוי בסיסי.\n",
        "    \"\"\"\n",
        "    # ניקוי בסיסי\n",
        "    answer = predicted_answer.strip()\n",
        "    answer = re.sub(r'\\[.*?\\]|\\(.*?\\)|\\.\\.\\.', '', answer)\n",
        "    answer = re.sub(r'^[\\\"\\']|[\\\"\\']$', '', answer)\n",
        "\n",
        "    # מחפש את אחת התשובות האמיתיות בתוך הטקסט\n",
        "    for true_ans in true_answers:\n",
        "        pattern = re.escape(true_ans.strip())\n",
        "        match = re.search(pattern, answer)\n",
        "        if match:\n",
        "            return match.group(0)  # מחזיר בדיוק את הטקסט שהתקבל\n",
        "    # אם לא נמצא, מחזיר את הטקסט שנוקה\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "PvobTxHb2edS"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "יש אופציה \"להעניש\" את המודל אם התשובה לא קיימת והמודל המציא תשובה, כרגע אנחנו פשוט נותנים ציון 0"
      ],
      "metadata": {
        "id": "fdzGBmxo_knK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "\n",
        "DATASET_PATH = \"hebrew_qa.json\"  # הנתונים שלך\n",
        "PERPLEXITY_MODEL = \"sonar-pro\"\n",
        "PERPLEXITY_URL = \"https://api.perplexity.ai/chat/completions\"\n",
        "PERPLEXITY_API_KEY  = \"YOUR API KEY\"\n",
        "SLEEP = 1.0\n",
        "\n",
        "# =========================\n",
        "# LOAD DATASET\n",
        "# =========================\n",
        "def load_dataset(path: str, limit: int = None) -> List[Dict]:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    samples = []\n",
        "    for entry in data[\"data\"]:\n",
        "        for para in entry[\"paragraphs\"]:\n",
        "            context = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "                samples.append({\n",
        "                    \"context\": context,\n",
        "                    \"question\": qa[\"question\"],\n",
        "                    \"answers\": [a[\"text\"] for a in qa.get(\"answers\", [])],\n",
        "                    \"is_impossible\": qa.get(\"is_impossible\", False)\n",
        "                })\n",
        "    if limit:\n",
        "        return samples[:limit]\n",
        "    return samples\n",
        "\n",
        "# =========================\n",
        "# CALL PERPLEXITY QA\n",
        "# =========================\n",
        "def call_perplexity_qa(context: str, question: str, few_shot_example: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "אתה מומחה ב-Hebrew QA. עקוב אחרי הפורמט:\n",
        "{few_shot_example}\n",
        "\n",
        "כעת הטקסט:\n",
        "Context: {context}\n",
        "\n",
        "שאלה: {question}\n",
        "\n",
        "אם אין תשובה בטקסט – כתוב 'Unanswerable'. אחרת – כתוב רק את ה-span המדויק מהטקסט.\n",
        "\"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {PERPLEXITY_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": PERPLEXITY_MODEL,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a precise Hebrew Extractive QA model with reasoning.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"temperature\": 0.0,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.post(PERPLEXITY_URL, headers=headers, json=payload, timeout=300)\n",
        "        r.raise_for_status()\n",
        "        text = r.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "        # חילוץ התשובה אחרי 'A:' או השורה האחרונה\n",
        "        match = re.search(r\"A\\s*:\\s*(.+)\", text, flags=re.DOTALL)\n",
        "        if match:\n",
        "            answer = match.group(1).strip()\n",
        "            answer = re.sub(r'^[\"\\']|[\"\\']$', '', answer)\n",
        "            return answer\n",
        "        lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
        "        if lines:\n",
        "            return lines[-1]\n",
        "        return \"Unanswerable\"\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ שגיאה בקריאה ל-Perplexity:\", e)\n",
        "        return \"Unanswerable\"\n",
        "\n",
        "# =========================\n",
        "# TLNLS METRIC (UPDATED)\n",
        "# =========================\n",
        "def levenshtein_distance(s: str, t: str) -> int:\n",
        "    m, n = len(s), len(t)\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            cost = 0 if s[i - 1] == t[j - 1] else 1\n",
        "            dp[i][j] = min(\n",
        "                dp[i - 1][j] + 1,\n",
        "                dp[i][j - 1] + 1,\n",
        "                dp[i - 1][j - 1] + cost\n",
        "            )\n",
        "    return dp[m][n]\n",
        "\n",
        "def normalized_lev_similarity(a: str, b: str) -> float:\n",
        "    if len(a) == 0 and len(b) == 0:\n",
        "        return 1.0\n",
        "    lev = levenshtein_distance(a, b)\n",
        "    return 1.0 - lev / max(len(a), len(b))\n",
        "\n",
        "def token_level_normalized_lev_similarity(pred: str, gold: str) -> float:\n",
        "    pred_tokens = pred.split()\n",
        "    gold_tokens = gold.split()\n",
        "    if len(pred_tokens) == 0 and len(gold_tokens) == 0:\n",
        "        return 1.0\n",
        "    total_similarity = 0.0\n",
        "    for g in gold_tokens:\n",
        "        best = 0.0\n",
        "        for p in pred_tokens:\n",
        "            score = normalized_lev_similarity(g, p)\n",
        "            if score > best:\n",
        "                best = score\n",
        "        total_similarity += best\n",
        "    norm_factor = max(len(gold_tokens), len(pred_tokens))\n",
        "    return total_similarity / norm_factor\n",
        "\n",
        "# =========================\n",
        "# RUN EVALUATION\n",
        "# =========================\n",
        "def run_hebrew_qa_evaluation(limit: int = 10):\n",
        "    dataset = load_dataset(DATASET_PATH, limit=limit)\n",
        "\n",
        "    few_shot_example = \"\"\"\n",
        "Context: \"הספרייה הלאומית הוקמה בשנת 1892 ומכילה אוספים רבים של ספרים ועיתונים.\"\n",
        "Q: מתי הוקמה הספרייה הלאומית?\n",
        "A: בשנת 1892\n",
        "\n",
        "Context: \"תכנית החלל של ישראל התחילה בשנת 1983 ונועדה לפתח לוויינים.\"\n",
        "Q: מהי מטרת תכנית החלל של ישראל?\n",
        "A: לפתח לוויינים\n",
        "\n",
        "Context: \"ביום ראשון התחוללה סערה קשה באזור הצפון.\"\n",
        "Q: מה קרה ביום ראשון באזור הצפון?\n",
        "A: סערה קשה\n",
        "\"\"\"\n",
        "    total_score = 0.0\n",
        "\n",
        "    for idx, sample in enumerate(dataset, 1):\n",
        "        context = sample[\"context\"]\n",
        "        question = sample[\"question\"]\n",
        "        true_answers = sample[\"answers\"] if not sample[\"is_impossible\"] else [\"Unanswerable\"]\n",
        "\n",
        "        #predicted_answer = call_perplexity_qa(context, question, few_shot_example)\n",
        "\n",
        "        # במידה רוצים לקבל את התשובה הנכונה מתוך התשובה הכוללת\n",
        "\n",
        "        predicted_raw = call_perplexity_qa(context, question, few_shot_example)\n",
        "        predicted_answer = extract_shortest_span(predicted_raw,true_answers)\n",
        "\n",
        "        # חישוב TLNLS עם הפונקציה המעודכנת\n",
        "        best_score = max([token_level_normalized_lev_similarity(predicted_answer, ta) for ta in true_answers])\n",
        "        total_score += best_score\n",
        "\n",
        "        print(f\"\\n===== Sample {idx} =====\")\n",
        "        print(\"Question:\", question)\n",
        "        print(\"True Answer(s):\", true_answers)\n",
        "        print(\"Predicted Answer:\", predicted_answer)\n",
        "        print(f\"TLNLS score: {best_score:.3f}\")\n",
        "\n",
        "    avg_score = total_score / len(dataset)\n",
        "    print(\"\\n===== FINAL TLNLS SCORE =====\")\n",
        "    print(f\"Average TLNLS: {avg_score:.3f}\")\n",
        "\n",
        "# =========================\n",
        "# RUN\n",
        "# =========================\n",
        "if __name__ == \"__main__\":\n",
        "    run_hebrew_qa_evaluation(limit=15)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0O5GeHAyKof",
        "outputId": "b1352b33-8b6c-44c6-f0db-90c61e68bdbf"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Sample 1 =====\n",
            "Question: כמה אנשים היו בוועדה שניסחה את הטיוטה הרביעית?\n",
            "True Answer(s): ['חמישה']\n",
            "Predicted Answer: חמישה\n",
            "TLNLS score: 1.000\n",
            "\n",
            "===== Sample 2 =====\n",
            "Question: באילו מילים התחילו כל סעיפי הטויוטה?\n",
            "True Answer(s): ['הואיל ו']\n",
            "Predicted Answer: הואיל ו\n",
            "TLNLS score: 1.000\n",
            "\n",
            "===== Sample 3 =====\n",
            "Question: איזו תוכנית הוכרזה כחלק מההצעה של הטיוטה?\n",
            "True Answer(s): ['תוכנית החלוקה של האו\"ם']\n",
            "Predicted Answer: תוכנית החלוקה של האו\"ם\n",
            "TLNLS score: 1.000\n",
            "\n",
            "===== Sample 4 =====\n",
            "Question: מי היה נשיא המדינה בזמן חתימת המגילה?\n",
            "True Answer(s): ['חיים ויצמן']\n",
            "Predicted Answer: Unanswerable\n",
            "TLNLS score: 0.000\n",
            "\n",
            "===== Sample 5 =====\n",
            "Question: שמו של איזה חותם מגילה מגיע מיד אחרי ויצמן בסדר אלפביתי?\n",
            "True Answer(s): ['מאיר וילנר']\n",
            "Predicted Answer: מאיר וילנר\n",
            "TLNLS score: 1.000\n",
            "\n",
            "===== Sample 6 =====\n",
            "Question: היכן חתם בן-גוריון על המגילה?\n",
            "True Answer(s): ['בראש החותמים']\n",
            "Predicted Answer: Unanswerable\n",
            "TLNLS score: 0.000\n",
            "\n",
            "===== Sample 7 =====\n",
            "Question: היכן היה ויצמן בזמן חתימת המגילה?\n",
            "True Answer(s): ['בארצות הברית']\n",
            "Predicted Answer: בארצות הברית\n",
            "TLNLS score: 1.000\n",
            "\n",
            "===== Sample 8 =====\n",
            "Question: איזו עיר הייתה נצורה בזמן חתימת המגילה?\n",
            "True Answer(s): ['ירושלים']\n",
            "Predicted Answer: ירושלים\n",
            "TLNLS score: 1.000\n",
            "\n",
            "===== Sample 9 =====\n",
            "Question: חבר באיזו מועצה היה ויצמן?\n",
            "True Answer(s): ['Unanswerable']\n",
            "Predicted Answer: הטקסט מציין שויצמן **לא היה** חבר מועצת העם , אך לא מציין לאיזו מועצה הוא כן היה חבר. הטקסט רק מסביר שיומיים או שלושה ימים לאחר הכרזת העצמאות, נבחר ויצמן לנשיא מועצת המדינה.\n",
            "TLNLS score: 0.000\n",
            "\n",
            "===== Sample 10 =====\n",
            "Question: באיזה מקום חתם אליהו דובקין על המגילה?\n",
            "True Answer(s): ['Unanswerable']\n",
            "Predicted Answer: ** טור הראשון של החותמים\n",
            "TLNLS score: 0.000\n",
            "\n",
            "===== Sample 11 =====\n",
            "Question: מתי לראשונה בהיסטוריה קמו המדינות הראשונות?\n",
            "True Answer(s): ['לפני כ-6000 שנה']\n",
            "Predicted Answer: לפני כ-6000 שנה\n",
            "TLNLS score: 1.000\n",
            "\n",
            "===== Sample 12 =====\n",
            "Question: היכן קמו המדינות הראשונות?\n",
            "True Answer(s): ['באזור המזרח התיכון, במסופוטמיה']\n",
            "Predicted Answer: באזור המזרח התיכון, במסופוטמיה\n",
            "TLNLS score: 1.000\n",
            "\n",
            "===== Sample 13 =====\n",
            "Question: מה היה המצב השלילי שגרם לרצון להקמת המדינות?\n",
            "True Answer(s): ['אי סדר פוליטי']\n",
            "Predicted Answer: אי סדר פוליטי\n",
            "TLNLS score: 1.000\n",
            "\n",
            "===== Sample 14 =====\n",
            "Question: איזה סוג מונרכיות התפתחו מהמונרכיות המוחלטות?\n",
            "True Answer(s): ['מונרכיות קונסטיטוציוניות (חוקתיות)']\n",
            "Predicted Answer: מונרכיות קונסטיטוציוניות \n",
            "TLNLS score: 0.778\n",
            "\n",
            "===== Sample 15 =====\n",
            "Question: בזמן המונרכיות החוקתיות, איזו אימפריה או גוף שלטו?\n",
            "True Answer(s): ['Unanswerable']\n",
            "Predicted Answer: Unanswerable\n",
            "TLNLS score: 1.000\n",
            "\n",
            "===== FINAL TLNLS SCORE =====\n",
            "Average TLNLS: 0.719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Adapt to vllm"
      ],
      "metadata": {
        "id": "AoDN6eYZWnO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import time\n",
        "from typing import List, Dict\n",
        "from openai import OpenAI\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "\n",
        "DATASET_PATH = \"hebrew_qa.json\"\n",
        "VLLM_BASE_URL = \"http://localhost:8000/v1\"\n",
        "MODEL_NAME = \"your-model-name\"   # למשל: aya-23, gemma, mistral וכו'\n",
        "SLEEP = 1.0\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=VLLM_BASE_URL,\n",
        "    api_key=\"EMPTY\"  # vLLM לא דורש מפתח, אבל הספרייה מחייבת ערך\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# LOAD DATASET\n",
        "# =========================\n",
        "\n",
        "def load_dataset(path: str, limit: int = None) -> List[Dict]:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    samples = []\n",
        "    for entry in data[\"data\"]:\n",
        "        for para in entry[\"paragraphs\"]:\n",
        "            context = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "                samples.append({\n",
        "                    \"context\": context,\n",
        "                    \"question\": qa[\"question\"],\n",
        "                    \"answers\": [a[\"text\"] for a in qa.get(\"answers\", [])],\n",
        "                    \"is_impossible\": qa.get(\"is_impossible\", False)\n",
        "                })\n",
        "\n",
        "    return samples[:limit] if limit else samples\n",
        "\n",
        "# =========================\n",
        "# CALL vLLM QA (OpenAI API)\n",
        "# =========================\n",
        "\n",
        "def call_vllm_qa(context: str, question: str, few_shot_example: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "אתה מודל Hebrew Extractive QA.\n",
        "\n",
        "{few_shot_example}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "שאלה:\n",
        "{question}\n",
        "\n",
        "אם אין תשובה בטקסט – כתוב \"Unanswerable\".\n",
        "אחרת – החזר רק את ה-span המדויק מתוך הטקסט, ללא הסברים.\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL_NAME,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a precise Hebrew Extractive QA model.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "            max_tokens=128,\n",
        "        )\n",
        "\n",
        "        text = response.choices[0].message.content.strip()\n",
        "\n",
        "        # ניקוי תשובה\n",
        "        lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
        "        return lines[-1] if lines else \"Unanswerable\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ שגיאה בקריאה ל-vLLM:\", e)\n",
        "        return \"Unanswerable\"\n",
        "\n",
        "# =========================\n",
        "# TLNLS METRIC\n",
        "# =========================\n",
        "\n",
        "def levenshtein_distance(s: str, t: str) -> int:\n",
        "    m, n = len(s), len(t)\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            cost = 0 if s[i - 1] == t[j - 1] else 1\n",
        "            dp[i][j] = min(\n",
        "                dp[i - 1][j] + 1,\n",
        "                dp[i][j - 1] + 1,\n",
        "                dp[i - 1][j - 1] + cost\n",
        "            )\n",
        "    return dp[m][n]\n",
        "\n",
        "def normalized_lev_similarity(a: str, b: str) -> float:\n",
        "    if not a and not b:\n",
        "        return 1.0\n",
        "    return 1.0 - levenshtein_distance(a, b) / max(len(a), len(b))\n",
        "\n",
        "def token_level_normalized_lev_similarity(pred: str, gold: str) -> float:\n",
        "    pred_tokens = pred.split()\n",
        "    gold_tokens = gold.split()\n",
        "\n",
        "    if not pred_tokens and not gold_tokens:\n",
        "        return 1.0\n",
        "\n",
        "    total = 0.0\n",
        "    for g in gold_tokens:\n",
        "        best = max(normalized_lev_similarity(g, p) for p in pred_tokens) if pred_tokens else 0.0\n",
        "        total += best\n",
        "\n",
        "    return total / max(len(gold_tokens), len(pred_tokens))\n",
        "\n",
        "# =========================\n",
        "# RUN EVALUATION\n",
        "# =========================\n",
        "\n",
        "def run_hebrew_qa_evaluation(limit: int = 10):\n",
        "    dataset = load_dataset(DATASET_PATH, limit)\n",
        "\n",
        "    few_shot_example = \"\"\"\n",
        "Context: הספרייה הלאומית הוקמה בשנת 1892.\n",
        "Q: מתי הוקמה הספרייה הלאומית?\n",
        "A: בשנת 1892\n",
        "\n",
        "Context: תכנית החלל של ישראל התחילה בשנת 1983.\n",
        "Q: מתי התחילה תכנית החלל של ישראל?\n",
        "A: בשנת 1983\n",
        "\"\"\"\n",
        "\n",
        "    total_score = 0.0\n",
        "\n",
        "    for idx, sample in enumerate(dataset, 1):\n",
        "        context = sample[\"context\"]\n",
        "        question = sample[\"question\"]\n",
        "        true_answers = [\"Unanswerable\"] if sample[\"is_impossible\"] else sample[\"answers\"]\n",
        "\n",
        "        predicted = call_vllm_qa(context, question, few_shot_example)\n",
        "        score = max(token_level_normalized_lev_similarity(predicted, gt) for gt in true_answers)\n",
        "\n",
        "        total_score += score\n",
        "\n",
        "        print(f\"\\n===== Sample {idx} =====\")\n",
        "        print(\"Question:\", question)\n",
        "        print(\"True Answer(s):\", true_answers)\n",
        "        print(\"Predicted Answer:\", predicted)\n",
        "        print(f\"TLNLS score: {score:.3f}\")\n",
        "\n",
        "        time.sleep(SLEEP)\n",
        "\n",
        "    print(\"\\n===== FINAL TLNLS SCORE =====\")\n",
        "    print(f\"Average TLNLS: {total_score / len(dataset):.3f}\")\n",
        "\n",
        "# =========================\n",
        "# RUN\n",
        "# =========================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_hebrew_qa_evaluation(limit=15)\n"
      ],
      "metadata": {
        "id": "E2VZAKXfWrXF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
